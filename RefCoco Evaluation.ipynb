{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mikel/miniconda3/envs/vilbert/lib/python3.6/site-packages/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mikel/miniconda3/envs/vilbert/lib/python3.6/site-packages/tensorpack/tfutils/optimizer.py:18: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mikel/miniconda3/envs/vilbert/lib/python3.6/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from vilbert.datasets import ConceptCapLoaderTrain, ConceptCapLoaderVal\n",
    "from vilbert.vilbert import VILBertForVLTasks, BertConfig, BertForMultiModalPreTraining\n",
    "from vilbert.task_utils import LoadDatasetEval\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from maskrcnn_benchmark.layers import nms\n",
    "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import argparse\n",
    "import glob\n",
    "from types import SimpleNamespace\n",
    "import pdb\n",
    "import _pickle as cPickle\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    MAX_SIZE = 1333\n",
    "    MIN_SIZE = 800\n",
    "\n",
    "    def __init__(self):\n",
    "        self.args = self.get_parser().parse_args()\n",
    "        self.detection_model = self._build_detection_model()\n",
    "\n",
    "        os.makedirs(self.args.output_folder, exist_ok=True)\n",
    "\n",
    "    def get_parser(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\n",
    "            \"--model_file\", default=None, type=str, help=\"Detectron model file\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--config_file\", default=None, type=str, help=\"Detectron config file\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--imdb_gt_file\",\n",
    "            default=None,\n",
    "            type=str,\n",
    "            help=\"Imdb file containing file path and bboxes.\",\n",
    "        )\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size\")\n",
    "        parser.add_argument(\n",
    "            \"--num_features\",\n",
    "            type=int,\n",
    "            default=100,\n",
    "            help=\"Number of features to extract.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--output_folder\", type=str, default=\"./output\", help=\"Output folder\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--feature_name\",\n",
    "            type=str,\n",
    "            help=\"The name of the feature to extract\",\n",
    "            default=\"fc6\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--confidence_threshold\",\n",
    "            type=float,\n",
    "            default=0,\n",
    "            help=\"Threshold of detection confidence above which boxes will be selected\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--background\",\n",
    "            action=\"store_true\",\n",
    "            help=\"The model will output predictions for the background class when set\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--partition\", type=int, default=0, help=\"Partition to download.\"\n",
    "        )\n",
    "        return parser\n",
    "\n",
    "    def _build_detection_model(self):\n",
    "        cfg.merge_from_file(self.args.config_file)\n",
    "        cfg.freeze()\n",
    "\n",
    "        model = build_detection_model(cfg)\n",
    "        checkpoint = torch.load(self.args.model_file, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "        load_state_dict(model, checkpoint.pop(\"model\"))\n",
    "\n",
    "        model.to(\"cuda\")\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def get_batch_proposals(self, images, im_scales, im_infos, proposals):\n",
    "        proposals_batch = []\n",
    "        \n",
    "        for idx, img_info in enumerate(im_infos):\n",
    "            boxes_tensor = torch.from_numpy(\n",
    "                proposals[idx][\"bbox\"][: int(proposals[idx][\"num_box\"]), 0:]\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            orig_image_size = (img_info[\"width\"], img_info[\"height\"])\n",
    "            boxes = BoxList(boxes_tensor, orig_image_size)\n",
    "            image_size = (images.image_sizes[idx][1], images.image_sizes[idx][0])\n",
    "            boxes = boxes.resize(image_size)\n",
    "            proposals_batch.append(boxes)\n",
    "        return proposals_batch\n",
    "\n",
    "    def _image_transform(self, path):\n",
    "        img = Image.open(path)\n",
    "        im = np.array(img).astype(np.float32)\n",
    "        # IndexError: too many indices for array, grayscale images\n",
    "        if len(im.shape) < 3:\n",
    "            im = np.repeat(im[:, :, np.newaxis], 3, axis=2)\n",
    "        im = im[:, :, ::-1]\n",
    "        im -= np.array([102.9801, 115.9465, 122.7717])\n",
    "        im_shape = im.shape\n",
    "        im_height = im_shape[0]\n",
    "        im_width = im_shape[1]\n",
    "        im_size_min = np.min(im_shape[0:2])\n",
    "        im_size_max = np.max(im_shape[0:2])\n",
    "\n",
    "        # Scale based on minimum size\n",
    "        im_scale = self.MIN_SIZE / im_size_min\n",
    "\n",
    "        # Prevent the biggest axis from being more than max_size\n",
    "        # If bigger, scale it down\n",
    "        if np.round(im_scale * im_size_max) > self.MAX_SIZE:\n",
    "            im_scale = self.MAX_SIZE / im_size_max\n",
    "\n",
    "        im = cv2.resize(\n",
    "            im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR\n",
    "        )\n",
    "        img = torch.from_numpy(im).permute(2, 0, 1)\n",
    "\n",
    "        im_info = {\"width\": im_width, \"height\": im_height}\n",
    "\n",
    "        return img, im_scale, im_info\n",
    "\n",
    "    def _process_feature_extraction(\n",
    "        self, output, im_scales, im_infos, feature_name=\"fc6\", conf_thresh=0\n",
    "    ):\n",
    "        batch_size = len(output[0][\"proposals\"])\n",
    "        n_boxes_per_image = [len(boxes) for boxes in output[0][\"proposals\"]]\n",
    "        score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
    "        score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
    "        feats = output[0][feature_name].split(n_boxes_per_image)\n",
    "        cur_device = score_list[0].device\n",
    "\n",
    "        feat_list = []\n",
    "        info_list = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
    "            scores = score_list[i]\n",
    "            max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
    "            conf_thresh_tensor = torch.full_like(max_conf, conf_thresh)\n",
    "            start_index = 1\n",
    "            # Column 0 of the scores matrix is for the background class\n",
    "            if self.args.background:\n",
    "                start_index = 0\n",
    "            for cls_ind in range(start_index, scores.shape[1]):\n",
    "                cls_scores = scores[:, cls_ind]\n",
    "                keep = nms(dets, cls_scores, 0.5)\n",
    "                max_conf[keep] = torch.where(\n",
    "                    # Better than max one till now and minimally greater than conf_thresh\n",
    "                    (cls_scores[keep] > max_conf[keep])\n",
    "                    & (cls_scores[keep] > conf_thresh_tensor[keep]),\n",
    "                    cls_scores[keep],\n",
    "                    max_conf[keep],\n",
    "                )\n",
    "\n",
    "            feat_list.append(feats[i])\n",
    "            num_boxes = len(feats[i])\n",
    "            bbox = output[0][\"proposals\"][i]\n",
    "            bbox = bbox.resize(((im_infos[i][\"width\"], im_infos[i][\"height\"])))\n",
    "            bbox = bbox.bbox\n",
    "            # Predict the class label using the scores\n",
    "            objects = torch.argmax(scores[:, start_index:], dim=1)\n",
    "\n",
    "            info_list.append(\n",
    "                {\n",
    "                    \"bbox\": bbox.cpu().numpy(),\n",
    "                    \"num_boxes\": num_boxes,\n",
    "                    \"objects\": objects.cpu().numpy(),\n",
    "                    \"image_width\": im_infos[i][\"width\"],\n",
    "                    \"image_height\": im_infos[i][\"height\"],\n",
    "                    \"cls_prob\": scores.cpu().numpy(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return feat_list, info_list\n",
    "\n",
    "    def get_detectron_features(self, image_paths):\n",
    "        img_tensor, im_scales, im_infos, im_bbox = [], [], [], []\n",
    "\n",
    "        for image_path in image_paths:\n",
    "            #print('Image Path ' ,image_path)\n",
    "            # print(\"image transformations...\")\n",
    "            im, im_scale, im_info = self._image_transform(image_path[\"file_path\"])\n",
    "            print(\"image transformations done\")\n",
    "            img_tensor.append(im)\n",
    "            im_scales.append(im_scale)\n",
    "            im_infos.append(im_info)\n",
    "            im_bbox.append(image_path)\n",
    "\n",
    "        # Image dimensions should be divisible by 32, to allow convolutions\n",
    "        # in detector to work\n",
    "        current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "        current_img_list = current_img_list.to(\"cuda\")\n",
    "        # print(\"Infos: curr image, im_scale, img_infos, image_path_bbox: \\n\",current_img_list, im_scales, im_infos, im_bbox )\n",
    "        # print(\"Getting batch proposals...\")\n",
    "        # print(\"Infos: curr image, im_scale, img_infos, image_path_bbox: \\n\",current_img_list, im_scales, im_infos, image_paths['bbox'] )\n",
    "        proposals = self.get_batch_proposals(\n",
    "            current_img_list, im_scales, im_infos, im_bbox\n",
    "        )\n",
    "        print(\"Getting batch proposals done\")\n",
    "  \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.detection_model(current_img_list, proposals=proposals)\n",
    "\n",
    "        feat_list = self._process_feature_extraction(\n",
    "            output,\n",
    "            im_scales,\n",
    "            im_infos,\n",
    "            self.args.feature_name,\n",
    "            self.args.confidence_threshold,\n",
    "        )\n",
    "        print(\"Features extracted!\")\n",
    "        return feat_list\n",
    "\n",
    "    def _chunks(self, array, chunk_size):\n",
    "        for i in range(0, len(array), chunk_size):\n",
    "            yield array[i : i + chunk_size]\n",
    "\n",
    "    def _save_feature(self, file_name, feature, info):\n",
    "        file_base_name = str(file_name).split(\".\")[0]\n",
    "        info[\"image_id\"] = file_base_name\n",
    "        info[\"features\"] = feature.cpu().numpy()\n",
    "        file_base_name = str(file_base_name) + \".npy\"\n",
    "\n",
    "        np.save(os.path.join(self.args.output_folder, file_base_name), info)\n",
    "        print(\"Saved in: \"+os.path.join(self.args.output_folder, file_base_name))\n",
    "\n",
    "    def extract_features(self):\n",
    "        files = np.load(self.args.imdb_gt_file, allow_pickle=True)\n",
    "        extracted_features = []\n",
    "        # files = sorted(files)\n",
    "        # files = [files[i: i+1000] for i in range(0, len(files), 1000)][self.args.partition]\n",
    "        cnt = 1\n",
    "        for chunk in self._chunks(files, self.args.batch_size):\n",
    "            try:\n",
    "                print('##############   CNT : ', cnt)\n",
    "                cnt += 1\n",
    "                print('Getting features...')\n",
    "                # print(chunk)\n",
    "                features, infos = self.get_detectron_features(chunk)\n",
    "                extracted_features.append((features, infos))\n",
    "                print('Getting batch features done!')\n",
    "            except BaseException:\n",
    "                continue\n",
    "        np.save('gt_feat.npy', extracted_features)\n",
    "        return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "def printer(sent, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent)[1:-1]\n",
    "    print(\" \".join(sent))\n",
    "\n",
    "def show_boxes2(img_path, boxes, colors, texts=None, masks=None):\n",
    "    # boxes [[xyxy]]\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    print('boxes: ',boxes)\n",
    "    for k in range(boxes.shape[0]):\n",
    "        box = boxes[k]\n",
    "        xmin, ymin, xmax, ymax = list(box)\n",
    "        coords = (xmin, ymin), xmax - xmin + 1, ymax - ymin + 1\n",
    "        color = colors[k]\n",
    "        ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "        if texts is not None:\n",
    "            ax.text(xmin, ymin, texts[k], bbox={'facecolor':'blue', 'alpha':0.5},fontsize=8, color='white')\n",
    "\n",
    "            \n",
    "# write arbitary string for given sentense. \n",
    "def plot_attention_maps(attn_maps, x_labels, y_labels, title, out_file, type):\n",
    "    # create a 1920 x 1080 pixel image\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(19.2, 10.8))\n",
    "\n",
    "    attn_head_idx = 0\n",
    "    for row in range(0, 2):\n",
    "        for col in range(0, 4):\n",
    "            ax[row][col].imshow(attn_maps[attn_head_idx])\n",
    "\n",
    "            ax[row][col].set_xticks(np.arange(len(x_labels)))\n",
    "            ax[row][col].set_xticklabels(x_labels)\n",
    "            if row == 0:\n",
    "                ax[row][col].xaxis.tick_top()\n",
    "                plt.setp(ax[row][col].get_xticklabels(), rotation=90, ha=\"left\", rotation_mode=\"anchor\")\n",
    "            else:\n",
    "                plt.setp(ax[row][col].get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "            # show y ticks only on left column\n",
    "            if col == 0:\n",
    "                ax[row][col].set_yticks(np.arange(len(y_labels)))\n",
    "                ax[row][col].set_yticklabels(y_labels)\n",
    "            else:\n",
    "                ax[row][col].set_yticks([])\n",
    "                ax[row][col].set_yticklabels([])\n",
    "\n",
    "            attn_head_idx += 1\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.text(24.25, 0, title, size=18, verticalalignment='center', rotation=270)\n",
    "\n",
    "    # move vision on text attention maps more to the top and text on vision attention maps to the bottom such that\n",
    "    # larger words fit into the visualization\n",
    "    if type == 'vis':\n",
    "        plt.subplots_adjust(left=0.1, right=0.98, top=1.0)\n",
    "    else:\n",
    "        plt.subplots_adjust(left=0.1, right=0.98, top=0.9, bottom=0.0)\n",
    "\n",
    "    plt.savefig(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_refering_expression(question, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task_tokens, ):\n",
    "\n",
    "    vil_prediction, vil_prediction_gqa, vil_logit, vil_binary_prediction, vil_tri_prediction, vision_prediction, vision_logit, linguisic_prediction, linguisic_logit, attn_data_list = model(\n",
    "        question, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task_tokens, output_all_attention_masks=True\n",
    "    )\n",
    "\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "\n",
    "    # grounding: \n",
    "    logits_vision = torch.max(vision_logit, 1)[1].data\n",
    "    grounding_val, grounding_idx = torch.sort(vision_logit.view(-1), 0, True)    \n",
    "    \n",
    "    \n",
    "    # for whole batch to do!\n",
    "    top_idx = grounding_idx[0]\n",
    "    print('top_idx: ',top_idx)\n",
    "    top_box = spatials[0][top_idx][:4].tolist() \n",
    "    y1 = int(top_box[1] * height)\n",
    "    y2 = int(top_box[3] * height)\n",
    "    x1 = int(top_box[0] * width)\n",
    "    x2 = int(top_box[2] * width)\n",
    "    \n",
    "    predicted_bboxes = [x1, y1, x2, y2]\n",
    "    return predicted_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prediction(query, task, features, infos):\n",
    "\n",
    "    tokens = tokenizer.encode(query)\n",
    "    tokens = tokenizer.add_special_tokens_single_sentence(tokens)\n",
    "    \n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_mask = [1] * len(tokens)\n",
    "\n",
    "    max_length = 37\n",
    "    if len(tokens) < max_length:\n",
    "        # Note here we pad in front of the sentence\n",
    "        padding = [0] * (max_length - len(tokens))\n",
    "        tokens = tokens + padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "    text = torch.from_numpy(np.array(tokens)).cuda().unsqueeze(0)\n",
    "    input_mask = torch.from_numpy(np.array(input_mask)).cuda().unsqueeze(0)\n",
    "    segment_ids = torch.from_numpy(np.array(segment_ids)).cuda().unsqueeze(0)\n",
    "    task = torch.from_numpy(np.array(task)).cuda().unsqueeze(0)\n",
    "\n",
    "    num_image = len(infos)\n",
    "\n",
    "    feature_list = []\n",
    "    image_location_list = []\n",
    "    image_mask_list = []\n",
    "    for i in range(num_image):\n",
    "        image_w = infos[i]['image_width']\n",
    "        image_h = infos[i]['image_height']\n",
    "        feature = features[i]\n",
    "        num_boxes = feature.shape[0]\n",
    "\n",
    "        g_feat = torch.sum(feature, dim=0) / num_boxes\n",
    "        num_boxes = num_boxes + 1\n",
    "        feature = torch.cat([g_feat.view(1,-1), feature], dim=0)\n",
    "        boxes = infos[i]['bbox']\n",
    "        image_location = np.zeros((boxes.shape[0], 5), dtype=np.float32)\n",
    "        image_location[:,:4] = boxes\n",
    "        image_location[:,4] = (image_location[:,3] - image_location[:,1]) * (image_location[:,2] - image_location[:,0]) / (float(image_w) * float(image_h))\n",
    "        image_location[:,0] = image_location[:,0] / float(image_w)\n",
    "        image_location[:,1] = image_location[:,1] / float(image_h)\n",
    "        image_location[:,2] = image_location[:,2] / float(image_w)\n",
    "        image_location[:,3] = image_location[:,3] / float(image_h)\n",
    "        g_location = np.array([0,0,1,1,1])\n",
    "        image_location = np.concatenate([np.expand_dims(g_location, axis=0), image_location], axis=0)\n",
    "        image_mask = [1] * (int(num_boxes))\n",
    "\n",
    "        feature_list.append(feature)\n",
    "        image_location_list.append(torch.tensor(image_location))\n",
    "        image_mask_list.append(torch.tensor(image_mask))\n",
    "\n",
    "    features = torch.stack(feature_list, dim=0).float().cuda()\n",
    "    spatials = torch.stack(image_location_list, dim=0).float().cuda()\n",
    "    image_mask = torch.stack(image_mask_list, dim=0).byte().cuda()\n",
    "    co_attention_mask = torch.zeros((num_image, num_boxes, max_length)).cuda()\n",
    "\n",
    "    predicted_bboxes = prediction_refering_expression(text, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_file MODEL_FILE]\n",
      "                             [--config_file CONFIG_FILE]\n",
      "                             [--imdb_gt_file IMDB_GT_FILE]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_features NUM_FEATURES]\n",
      "                             [--output_folder OUTPUT_FOLDER]\n",
      "                             [--feature_name FEATURE_NAME]\n",
      "                             [--confidence_threshold CONFIDENCE_THRESHOLD]\n",
      "                             [--background] [--partition PARTITION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/mikel/.local/share/jupyter/runtime/kernel-7d8dd4d1-c463-4d06-b6c1-08d9a72b29a5.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tools.refer.refer import REFER\n",
    "import numpy as np\n",
    "import sys\n",
    "import os.path as osp\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset refcoco into memory...\n",
      "creating index...\n",
      "index created.\n",
      "DONE (t=7.41s)\n"
     ]
    }
   ],
   "source": [
    "data_root = './data'  # contains refclef, refcoco, refcoco+, refcocog and images\n",
    "dataset = 'refcoco'\n",
    "splitBy = 'unc'\n",
    "refer = REFER(data_root, dataset, splitBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric\n",
    "def computeIoU(box1, box2):\n",
    "    # each box is of [x1, y1, w, h]\n",
    "    inter_x1 = max(box1[0], box2[0])\n",
    "    inter_y1 = max(box1[1], box2[1])\n",
    "    inter_x2 = min(box1[0]+box1[2]-1, box2[0]+box2[2]-1)\n",
    "    inter_y2 = min(box1[1]+box1[3]-1, box2[1]+box2[3]-1)\n",
    "\n",
    "    if inter_x1 < inter_x2 and inter_y1 < inter_y2:\n",
    "        inter = (inter_x2-inter_x1+1)*(inter_y2-inter_y1+1)\n",
    "    else:\n",
    "        inter = 0\n",
    "    union = box1[2]*box1[3] + box2[2]*box2[3] - inter\n",
    "    return float(inter)/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refering Expression Evaluator\n",
    "query = \"swimming elephant\"\n",
    "task = [9]\n",
    "predicted_bboxes = custom_prediction(query, task, features, infos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('vilbert': conda)",
   "language": "python",
   "name": "python361064bitvilbertconda392d4eabd7b74bf6aae5c7f2c0559d0e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
